{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/fvvTTINcw+d2ed/JJiba",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/binoyskaria/google_collab_projects/blob/main/q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN3POmdCPCMT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/Q1Data.csv\")\n",
        "df = df.sample(frac=0.6, random_state=42)\n",
        "\n",
        "# Assuming the MFCC features are in columns 1 to 21 and the target variable (Genus) is in the last column\n",
        "X = df.iloc[:, 1:-3].values\n",
        "y = df['Genus'].values\n",
        "\n",
        "# Convert categorical labels to numeric labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the dataset into train and test sets (80:20 ratio)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Part 1: Scratch Decision Tree\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
        "        self.min_samples_split=min_samples_split\n",
        "        self.max_depth=max_depth\n",
        "        self.n_features=n_features\n",
        "        self.root=None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
        "        self.root = self._grow_tree(X, y)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_feats = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        # check the stopping criteria\n",
        "        if (depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
        "\n",
        "        # find the best split\n",
        "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
        "\n",
        "        # create child nodes\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n",
        "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
        "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
        "        return Node(best_feature, best_thresh, left, right)\n",
        "\n",
        "\n",
        "    def _best_split(self, X, y, feat_idxs):\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            thresholds = np.unique(X_column)\n",
        "\n",
        "            for thr in thresholds:\n",
        "                # calculate the information gain\n",
        "                gain = self._information_gain(y, X_column, thr)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_threshold = thr\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "\n",
        "    def _information_gain(self, y, X_column, threshold):\n",
        "        # parent entropy\n",
        "        parent_entropy = self._entropy(y)\n",
        "\n",
        "        # create children\n",
        "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
        "\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "            return 0\n",
        "\n",
        "        # calculate the weighted avg. entropy of children\n",
        "        n = len(y)\n",
        "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
        "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
        "        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
        "\n",
        "        # calculate the IG\n",
        "        information_gain = parent_entropy - child_entropy\n",
        "        return information_gain\n",
        "\n",
        "    def _split(self, X_column, split_thresh):\n",
        "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        hist = np.bincount(y)\n",
        "        ps = hist / len(y)\n",
        "        return -np.sum([p * np.log(p) for p in ps if p>0])\n",
        "\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        counter = Counter(y)\n",
        "        value = counter.most_common(1)[0][0]\n",
        "        return value\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)\n",
        "# Part 1a: Train a Decision Tree Classifier\n",
        "print(\"Decision Tree Classifier trained.\")\n",
        "\n",
        "# Part 1b: Plot the test accuracy by pruning the tree to a depth ranging from 1 to 15\n",
        "depths = list(range(1, 16))\n",
        "test_accuracies_1 = []\n",
        "\n",
        "for depth in depths:\n",
        "    # Prune the tree to the specified depth\n",
        "    tree_classifier = DecisionTree(min_samples_split=5, max_depth=depth, n_features=21)\n",
        "    tree_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    predictions = tree_classifier.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = np.mean(predictions == y_test)\n",
        "    test_accuracies_1.append(accuracy)\n",
        "\n",
        "# Plotting\n",
        "# plt.plot(depths, test_accuracies_1, marker='o')\n",
        "# plt.title('Test Accuracy vs. Tree Depth')\n",
        "# plt.xlabel('Tree Depth')\n",
        "# plt.ylabel('Test Accuracy')\n",
        "# plt.grid(True)\n",
        "\n",
        "# Part 1c: Observations\n",
        "# Your observations can be made based on the plotted graph. Look for the point where the test accuracy is optimal.\n",
        "# This is usually the point before overfitting occurs. Observe how the test accuracy changes with different depths.\n",
        "\n",
        "#Part 2: Scratch Knn\n",
        "\n",
        "def euclidean_distance(x1, x2):\n",
        "    distance = np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "    return distance\n",
        "\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = [self._predict(x) for x in X]\n",
        "        return predictions\n",
        "\n",
        "    def _predict(self, x):\n",
        "        # compute the distance\n",
        "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "\n",
        "        # get the closest k\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "\n",
        "        # majority vote\n",
        "        most_common = Counter(k_nearest_labels).most_common()\n",
        "        return most_common[0][0]\n",
        "\n",
        "# Part 2a: Train a kNN Classifier\n",
        "\n",
        "print(\"kNN Classifier trained.\")\n",
        "\n",
        "# Part 2b: Plot test accuracy for k varying from 1 to 1000\n",
        "max_k = 50\n",
        "k_values = list(range(1, max_k + 1))\n",
        "test_accuracies_2 = []\n",
        "\n",
        "for k in k_values:\n",
        "    knn_classifier = KNN(k)\n",
        "    knn_classifier.fit(X_train, y_train)\n",
        "    predictions = knn_classifier.predict(X_test)\n",
        "    accuracy = np.mean(predictions == y_test)\n",
        "    test_accuracies_2.append(accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# Plotting\n",
        "# plt.plot(k_values, test_accuracies_2, marker='o')\n",
        "# plt.title('Test Accuracy vs. k for kNN')\n",
        "# plt.xlabel('k (Number of Neighbors)')\n",
        "# plt.ylabel('Test Accuracy')\n",
        "# plt.grid(True)\n",
        "\n",
        "# Part 2c: Observations\n",
        "# Your observations can be made based on the plotted graph. Look for the point where the test accuracy is optimal.\n",
        "# This is usually the point before overfitting occurs. Observe how the test accuracy changes with different k values.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Part 3a: Decision Trees using scikit-learn\n",
        "depths = list(range(1, 16))\n",
        "dt_test_accuracies = []\n",
        "\n",
        "for depth in depths:\n",
        "    dt_classifier = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    dt_classifier.fit(X_train, y_train)\n",
        "    dt_predictions = dt_classifier.predict(X_test)\n",
        "    dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
        "    dt_test_accuracies.append(dt_accuracy)\n",
        "\n",
        "# Part 3b: kNN using scikit-learn\n",
        "max_k = 1000\n",
        "k_values = list(range(1, max_k + 1))\n",
        "knn_test_accuracies = []\n",
        "\n",
        "for k in k_values:\n",
        "    knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn_classifier.fit(X_train, y_train)\n",
        "    knn_predictions = knn_classifier.predict(X_test)\n",
        "    knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
        "    knn_test_accuracies.append(knn_accuracy)\n",
        "\n",
        "# Part 3c: Plotting\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot for Decision Trees\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(depths, dt_test_accuracies, marker='o')\n",
        "plt.title('Decision Tree Test Accuracy vs. Depth')\n",
        "plt.xlabel('Tree Depth')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot for kNN\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(k_values, knn_test_accuracies, marker='o')\n",
        "plt.title('kNN Test Accuracy vs. k')\n",
        "plt.xlabel('k (Number of Neighbors)')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.grid(True)\n",
        "\n",
        "# # Part 3d: Comparing models built from Scratch and scikit-learn\n",
        "# # Plot for Decision Trees (built from Scratch)\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(depths, test_accuracies_1, marker='o', label='Scratch')\n",
        "plt.title('Decision Tree (Scratch) Test Accuracy vs. Depth')\n",
        "plt.xlabel('Tree Depth')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot for kNN (built from Scratch)\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(k_values, test_accuracies_2, marker='o', label='Scratch')\n",
        "plt.title('kNN (Scratch) Test Accuracy vs. k')\n",
        "plt.xlabel('k (Number of Neighbors)')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ]
}